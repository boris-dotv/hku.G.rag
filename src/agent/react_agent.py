"""
Enhanced Agentic RAG with Query Rewriting and ReAct Loop

æ ¸å¿ƒç‰¹æ€§ï¼š
1. Query Rewriting: åŸºäºå¯¹è¯å†å²é‡å†™æŸ¥è¯¢ï¼ˆè§£å†³æŒ‡ä»£æ¶ˆè§£ï¼‰
2. ReAct Loop: Thought â†’ Action â†’ Observation å¾ªç¯
3. è®°å¿†ç³»ç»Ÿ: çŸ­æœŸ + é•¿æœŸè®°å¿†
4. å·¥å…·è°ƒç”¨: å‘é‡æ£€ç´¢
"""

import sys
import os
sys.path.append('src/rag')

import json
import requests
import re
import time
import signal
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from pipeline import RAGPipeline, RAGConfig
from .memory import MemorySystem, MemoryItem
import numpy as np


# ============================================
# Timeout Exception
# ============================================
class TimeoutError(Exception):
    """Exception raised when operation times out"""
    pass


def timeout_handler(signum, frame):
    raise TimeoutError("Operation timed out")


# ============================================
# Query Rewriter
# ============================================
class QueryRewriter:
    """æŸ¥è¯¢é‡å†™å™¨ï¼šåŸºäºå¯¹è¯å†å²è§£å†³æŒ‡ä»£æ¶ˆè§£"""

    def __init__(self, glm_key: str):
        self.glm_key = glm_key
        self.api_url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"

    def rewrite(self, current_query: str, conversation_history: List[Dict]) -> str:
        """
        é‡å†™æŸ¥è¯¢ï¼Œè§£æä»£è¯å’Œçœç•¥ï¼ˆæ¿€è¿›ç­–ç•¥ï¼šæœ‰å†å²å°±æ€»æ˜¯å°è¯•é‡å†™ï¼‰

        Args:
            current_query: å½“å‰ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²

        Returns:
            é‡å†™åçš„æŸ¥è¯¢
        """
        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ï¼šåªè¦å†å²ä¸ä¸ºç©ºï¼Œå°±å¼ºåˆ¶è®© LLM å®¡è§†ä¸€æ¬¡æŸ¥è¯¢
        # ä¸è¦ç›¸ä¿¡é¢„åˆ¤ï¼Œè®© LLM å†³å®šæ˜¯å¦éœ€è¦ä¿®æ”¹
        if not conversation_history:
            return current_query

        # æ„å»ºå†å²ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æœ€è¿‘ä¸€è½®ï¼Œå› ä¸ºQ4åªä¾èµ–Q3ï¼‰
        last_turn = conversation_history[-1]
        # _get_conversation_history() è¿”å›å­—å…¸ï¼Œç”¨ get() è®¿é—®
        context_query = last_turn.get('query', '')
        context_answer = last_turn.get('answer', '')[:300] if last_turn.get('answer') else ""  # åªç”¨å‰300å­—

        # ã€æ›´å¼ºçš„ Promptã€‘ï¼šæ˜ç¡®å¼ºè°ƒä»£è¯æ£€æµ‹
        prompt = f"""You are a conversation context resolver. Your ONLY job is to resolve pronouns and implicit references.

Current Query: "{current_query}"

Context from PREVIOUS turn:
- Previous User Question: "{context_query}"
- Previous Answer: "{context_answer}..."

CRITICAL TASK:
1. Does the Current Query contain pronouns (it, they, this, that, its, their, them)?
2. If YES, you MUST replace the pronoun with the actual topic from the Context.
3. If NO, output the Current Query exactly as-is.

Examples:
- Current: "How can it be prevented?" + Context about "overfitting" â†’ "How can overfitting be prevented?"
- Current: "What are its limitations?" + Context about "deep learning" â†’ "What are the limitations of deep learning?"
- Current: "Explain the process" + Context about "data splitting" â†’ "Explain the data splitting process"

Output ONLY the rewritten query. No explanations, no quotes:"""

        try:
            headers = {"Authorization": f"Bearer {self.glm_key}"}
            payload = {
                "model": RAGConfig.CHAT_MODEL,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 100,
                "temperature": 0.1  # é™ä½æ¸©åº¦è®©è¾“å‡ºæ›´ç¡®å®š
            }
            res = requests.post(self.api_url, headers=headers, json=payload, timeout=30)
            res.raise_for_status()
            data = res.json()

            if "choices" in data:
                rewritten = data["choices"][0]["message"]["content"].strip()
                # æ¸…ç†å¯èƒ½çš„å¼•å·å’Œå¤šä½™å­—ç¬¦
                rewritten = rewritten.strip('"\'').strip()

                # åªè¦æœ‰æ”¹åŠ¨ï¼Œå°±ä½¿ç”¨é‡å†™åçš„ç‰ˆæœ¬
                if rewritten and rewritten.lower() != current_query.lower():
                    print(f"  [ğŸ”„ Query Rewriting] '{current_query}' â†’ '{rewritten}'")
                    return rewritten
                else:
                    print(f"  [âœ“ Query Rewriting] No change needed for '{current_query}'")
                    return current_query

        except Exception as e:
            print(f"  [âš ï¸ Query Rewriting Error: {e}. Using original query.]")

        return current_query

    def _check_needs_rewriting(self, query: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å†™"""
        # æ£€æŸ¥ä»£è¯
        pronouns = ['it', 'its', 'they', 'them', 'their', 'this', 'that', 'these', 'those']
        query_lower = query.lower()

        # æ£€æŸ¥æ˜¯å¦æœ‰ä»£è¯å¼€å¤´
        words = query_lower.split()
        if words and words[0] in pronouns:
            return True

        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»£è¯ï¼ˆæ›´å®½æ¾çš„æ£€æŸ¥ï¼Œæ”¯æŒ "How can it be prevented?"ï¼‰
        if any(pronoun in query_lower for pronoun in pronouns):
            return True

        # æ£€æŸ¥æ˜¯å¦å¤ªçŸ­ï¼ˆå¯èƒ½æ˜¯çœç•¥ï¼‰
        if len(query.split()) <= 3 and not query.endswith('?'):
            return True

        return False

    def _format_history(self, history: List[Dict]) -> str:
        """æ ¼å¼åŒ–å†å² - å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦ä»¥æ›´å¥½åœ°è§£æä»£è¯"""
        lines = []
        for i, turn in enumerate(history, 1):
            query = turn.get('query', '')
            answer = turn.get('answer', '')
            # Include more context (200 chars) and the full last query
            if i == len(history):  # Most recent query - include full answer
                lines.append(f"Q{i}: {query}")
                lines.append(f"A{i}: {answer[:300]}...")
            else:  # Older queries - shorter context
                lines.append(f"Q{i}: {query}")
                lines.append(f"A{i}: {answer[:150]}...")
        return '\n'.join(lines)


# ============================================
# Query Decomposer
# ============================================
class QueryDecomposer:
    """æŸ¥è¯¢æ‹†è§£å™¨ï¼šä½¿ç”¨LLMå°†å¤æ‚æŸ¥è¯¢æ‹†è§£ä¸ºå¤šä¸ªå­æŸ¥è¯¢"""

    def __init__(self, glm_key: str):
        self.glm_key = glm_key
        self.api_url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"

    def decompose(self, query: str) -> Tuple[bool, List[str]]:
        """
        ä½¿ç”¨LLMåˆ¤æ–­å¹¶æ‹†è§£å¤æ‚æŸ¥è¯¢

        Returns:
            (should_decompose, sub_queries)
            - should_decompose: æ˜¯å¦éœ€è¦æ‹†è§£
            - sub_queries: æ‹†è§£åçš„å­æŸ¥è¯¢åˆ—è¡¨
        """
        prompt = f"""You are a query decomposition assistant. Analyze the user's query and determine if it needs to be broken down into multiple search queries.

User Query: {query}

Analyze the query:
1. If it compares two or more things (e.g., "difference between X and Y"), create separate queries for each
2. If it has multiple distinct questions, break them down
3. If it's simple and direct, keep it as-is

Output ONLY a JSON list of strings. For example:
- Simple query: ["original query"]
- Comparison: ["query about first topic", "query about second topic", "comparison query"]
- Multi-part: ["first part", "second part"]

JSON:"""

        try:
            headers = {"Authorization": f"Bearer {self.glm_key}"}
            payload = {
                "model": RAGConfig.CHAT_MODEL,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 300,
                "temperature": 0.3
            }
            res = requests.post(self.api_url, headers=headers, json=payload, timeout=30)
            res.raise_for_status()
            data = res.json()

            if "choices" in data:
                response = data["choices"][0]["message"]["content"].strip()

                # Try to extract JSON from response (LLM might add extra text)
                if not response:
                    print(f"    [Query decomposition: Empty response. Using original query.]")
                    return False, [query]

                # Find JSON array in response
                json_start = response.find('[')
                json_end = response.rfind(']') + 1

                if json_start >= 0 and json_end > json_start:
                    json_str = response[json_start:json_end]
                    sub_queries = json.loads(json_str)
                else:
                    # No JSON array found, try parsing whole response
                    sub_queries = json.loads(response)

                # Validate that we got a list of strings
                if isinstance(sub_queries, list) and all(isinstance(q, str) for q in sub_queries):
                    # Check if decomposition is needed (more than 1 query or different from original)
                    if len(sub_queries) > 1:
                        return True, sub_queries
                    elif len(sub_queries) == 1 and sub_queries[0].lower() != query.lower():
                        # LLM rephrased the query, use it
                        return False, sub_queries
                    else:
                        return False, [query]
                else:
                    print(f"    [Query decomposition: Invalid format. Using original query.]")
                    return False, [query]

        except json.JSONDecodeError as e:
            print(f"    [Query decomposition JSON error: {e}. Using original query.]")
        except Exception as e:
            print(f"    [Query decomposition error: {e}. Using original query.]")

        # Fallback: don't decompose
        return False, [query]


# ============================================
# Enhanced Agent with ReAct
# ============================================
class ReActAgent:
    """
    ReAct Agent: æ¨ç† + è¡ŒåŠ¨

    å¾ªç¯:
    1. Thought: åˆ†æå½“å‰æƒ…å†µï¼Œå†³å®šä¸‹ä¸€æ­¥
    2. Action: æ‰§è¡Œå·¥å…·æˆ–ç›´æ¥å›ç­”
    3. Observation: è§‚å¯Ÿç»“æœï¼Œå†³å®šæ˜¯å¦ç»§ç»­
    """

    def __init__(self,
                 pipeline: RAGPipeline,
                 glm_key: str,
                 enable_memory: bool = True,
                 enable_react: bool = True,
                 max_iterations: int = 3):
        self.pipeline = pipeline
        self.glm_key = glm_key
        self.enable_memory = enable_memory
        self.enable_react = enable_react
        self.max_iterations = max_iterations
        self.api_url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"

        # åˆå§‹åŒ–ç»„ä»¶
        self.query_rewriter = QueryRewriter(glm_key)
        self.query_decomposer = QueryDecomposer(glm_key)  # æ–°å¢ï¼šæŸ¥è¯¢æ‹†è§£å™¨

        if enable_memory:
            self.memory = MemorySystem(
                short_term_size=10,
                long_term_path="data/memory/memories.json"
            )
        else:
            self.memory = None

        # å·¥å…·æ³¨å†Œ
        self.tools = {
            "vector_search": self._vector_search,
            "bm25_search": self._bm25_search,
        }

    def query(self,
             user_query: str,
             mode: str = "hybrid",
             use_react: bool = True,
             use_memory: bool = True,
             save_to_memory: bool = True,
             importance: float = 0.5,
             verbose: bool = True) -> Dict:
        """
        æ‰§è¡ŒæŸ¥è¯¢ï¼ˆå¸¦ ReAct å¾ªç¯ï¼‰

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            mode: æ£€ç´¢æ¨¡å¼
            use_react: æ˜¯å¦ä½¿ç”¨ ReAct å¾ªç¯
            use_memory: æ˜¯å¦ä½¿ç”¨è®°å¿†
            save_to_memory: æ˜¯å¦ä¿å­˜åˆ°è®°å¿†
            importance: é‡è¦æ€§åˆ†æ•°
            verbose: æ˜¯å¦æ‰“å°æ€è€ƒè¿‡ç¨‹

        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        if verbose:
            print(f"\n{'='*70}")
            print(f"ğŸ¤– ReAct Agent Processing: {user_query}")
            print(f"{'='*70}")

        # ============ Step 1: Query Rewriting ============
        rewritten_query = user_query
        # Query rewriting can work independently of memory saving
        # It only needs conversation history for context
        if self.memory:
            history = self._get_conversation_history()
            rewritten_query = self.query_rewriter.rewrite(user_query, history)

        # ============ Step 2: ReAct Loop ============
        if use_react and self.enable_react:
            result = self._react_loop(rewritten_query, mode, verbose)
        else:
            # ç›´æ¥æŸ¥è¯¢
            result = self._direct_query(rewritten_query, mode)

        # ============ Step 3: Save to Memory ============
        if save_to_memory and self.memory:
            self._save_to_memory(
                user_query,
                rewritten_query,
                result["answer"],
                result.get("tools_used", []),
                importance
            )

        # æ·»åŠ å…ƒæ•°æ®
        result["original_query"] = user_query
        result["rewritten_query"] = rewritten_query
        result["query_was_rewritten"] = (user_query != rewritten_query)

        return result

    # ============================================
    # New Helper Methods for Enhanced ReAct
    # ============================================

    def _check_semantic_relevance(self, query: str, observation: Dict) -> Dict:
        """
        è¯­ä¹‰ç›¸å…³æ€§æ ¡éªŒï¼šæ£€æŸ¥æ£€ç´¢ç»“æœæ˜¯å¦çœŸçš„å›ç­”äº†é—®é¢˜

        ä¸åªçœ‹é•¿åº¦ï¼Œè€Œæ˜¯ç”¨LLMåˆ¤æ–­è¯­ä¹‰æ˜¯å¦ç›¸å…³
        """
        top_chunks = observation.get("sources", [])[:3]
        if not top_chunks:
            return {"is_satisfactory": False, "issue": "no_sources", "summary": "No sources"}

        chunks_text = "\n\n".join([f"- {c.get('content', '')[:200]}" for c in top_chunks])

        prompt = f"""You are a relevance checker. Determine if the retrieved content actually answers the question.

Question: {query}

Retrieved Content:
{chunks_text}

Strictly evaluate:
- If the content is about a completely different topic (e.g., aerodynamics for a data question), respond "No"
- If the content mentions keywords but doesn't actually answer, respond "Partial"
- Only respond "Yes" if it directly addresses the question

Answer (just one word: Yes/No/Partial):"""

        try:
            headers = {"Authorization": f"Bearer {self.glm_key}"}
            payload = {
                "model": RAGConfig.CHAT_MODEL,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 10,
                "temperature": 0.1
            }
            res = requests.post(self.api_url, headers=headers, json=payload, timeout=30)
            res.raise_for_status()
            data = res.json()

            if "choices" in data:
                answer = data["choices"][0]["message"]["content"].strip().lower()

                if "no" in answer:
                    return {
                        "is_satisfactory": False,
                        "issue": "semantic_mismatch",
                        "summary": "Retrieved content is semantically irrelevant"
                    }
                elif "partial" in answer:
                    return {
                        "is_satisfactory": False,
                        "issue": "partial_match",
                        "summary": "Retrieved content only partially relevant"
                    }

        except Exception as e:
            print(f"    [Semantic check error: {e}. Continuing...]")

        return {"is_satisfactory": True, "summary": "Semantically relevant"}

    def _expand_query(self, original_query: str, context: Dict) -> str:
        """
        æŸ¥è¯¢æ‰©å±•ï¼šå½“æ£€ç´¢å¤±è´¥æ—¶ï¼Œç”Ÿæˆæ›´å¤šæ ·åŒ–çš„æœç´¢è¯
        """
        # ä»ä¹‹å‰çš„observationä¸­æå–å…³é”®è¯
        keywords = set()
        if context["observations"]:
            last_obs = context["observations"][-1]
            sources = last_obs.get("sources", [])
            for source in sources[:2]:
                content = source.get("content", "")
                # ç®€å•çš„n-gramå…³é”®è¯æå–ï¼ˆå–2-3è¯çš„çŸ­è¯­ï¼‰
                words = content.lower().split()
                for i in range(len(words) - 1):
                    if len(words[i]) > 3:  # åªå–é•¿è¯
                        keywords.add(words[i])
                        if i < len(words) - 1 and len(words[i+1]) > 3:
                            keywords.add(f"{words[i]} {words[i+1]}")

        # æ„å»ºæ‰©å±•æŸ¥è¯¢
        if keywords:
            top_keywords = list(keywords)[:5]
            return f"{original_query} {' '.join(top_keywords[:3])}"

        return original_query

    def _extract_keywords(self, text: str, top_n: int = 5) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰"""
        # ç§»é™¤åœç”¨è¯
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'is', 'are', 'was', 'were'}
        words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())
        keywords = [w for w in words if w not in stopwords and len(w) >= 4]

        # ç»Ÿè®¡è¯é¢‘
        from collections import Counter
        word_freq = Counter(keywords)

        return [w for w, _ in word_freq.most_common(top_n)]

    def _react_loop(self, query: str, mode: str, verbose: bool) -> Dict:
        """ReAct å¾ªç¯ï¼šThought â†’ Action â†’ Observation â†’ Reflection

        æ”¯æŒæŸ¥è¯¢æ‹†è§£ï¼ˆQuery Decompositionï¼‰æ¥å¤„ç†å¤æ‚é—®é¢˜
        æ”¯æŒä¸Šä¸‹æ–‡æ³¨å…¥ï¼ˆContext Injectionï¼‰æ¥ä¿æŒå¯¹è¯è¿ç»­æ€§
        Hard timeout: 120 seconds max to prevent infinite loops
        """

        context = {
            "query": query,
            "thoughts": [],
            "actions": [],
            "observations": [],
            "reflections": [],
            "final_answer": "",
            "sub_queries": [],  # æ–°å¢ï¼šè®°å½•å­æŸ¥è¯¢
            "previous_topic": ""  # æ–°å¢ï¼šè®°å½•ä¸Šä¸€è½®çš„ä¸»é¢˜
        }

        # ============================================
        # Step -1: è·å–ä¸Šä¸€è½®çš„ä¸»é¢˜ï¼ˆä¸Šä¸‹æ–‡æ³¨å…¥ï¼‰
        # ============================================
        if self.memory:
            # è·å–æœ€è¿‘1è½®çš„å¯¹è¯å†å²
            recent_history = self.memory.short_term.get_recent(1)
            if recent_history:
                prev_query = recent_history[0].query  # MemoryItem ç”¨å±æ€§è®¿é—®
                # æå–å…³é”®è¯ä½œä¸ºä¸»é¢˜
                keywords = self._extract_keywords(prev_query, top_n=3)
                if keywords:
                    context["previous_topic"] = f"Previous topic: {', '.join(keywords)}"
                    if verbose:
                        print(f"  [Context] {context['previous_topic']}")

        # ============================================
        # Step 0: æ£€æŸ¥æ˜¯å¦éœ€è¦æ‹†è§£æŸ¥è¯¢ï¼ˆQuery Decompositionï¼‰
        # ============================================
        should_decompose, sub_queries = self.query_decomposer.decompose(query)

        if should_decompose:
            if verbose:
                print(f"\n  [ğŸ” Complex query detected. Decomposing into {len(sub_queries)} sub-queries:]")
                for i, sq in enumerate(sub_queries, 1):
                    print(f"      {i}. {sq}")

            context["sub_queries"] = sub_queries

            # å¯¹æ¯ä¸ªå­æŸ¥è¯¢æ‰§è¡Œæ£€ç´¢
            for i, sub_query in enumerate(sub_queries, 1):
                if verbose:
                    print(f"\n  [Sub-query {i}/{len(sub_queries)}] Searching: '{sub_query}'")

                observation = self._vector_search(sub_query, mode=mode)

                # ä¸ºæ¯ä¸ªå­æŸ¥è¯¢çš„observationæ‰“ä¸Šæ ‡ç­¾
                observation["sub_query"] = sub_query
                observation["sub_query_index"] = i

                context["observations"].append(observation)
                context["actions"].append("search")

                if verbose:
                    print(f"      ğŸ“Š Found {len(observation.get('sources', []))} chunks")

            # æ‹†è§£åçš„æŸ¥è¯¢ç›´æ¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼ˆè·³è¿‡æ ‡å‡†ReActå¾ªç¯ï¼‰
            context["final_answer"] = self._generate_final_answer(query, context)

            return {
                "answer": context["final_answer"],
                "sources": self._extract_all_sources(context),
                "react_steps": len(sub_queries),
                "reflections_count": 0,
                "thoughts": [f"Decomposed query into {len(sub_queries)} sub-queries"],
                "tools_used": ["search"],
                "was_decomposed": True,
                "sub_queries": sub_queries
            }

        # ============================================
        # æ ‡å‡†ReActå¾ªç¯ï¼ˆä¸éœ€è¦æ‹†è§£çš„æƒ…å†µï¼‰
        # ============================================
        def run_with_timeout():
            loop_start = time.time()

            for iteration in range(1, self.max_iterations + 1):
                # Check if we've exceeded the timeout
                elapsed = time.time() - loop_start
                if elapsed > 120:
                    print(f"    [âš ï¸  ReAct loop timeout after {elapsed:.1f}s, forcing answer...]")
                    break

                if verbose:
                    print(f"\n  [Iteration {iteration}/{self.max_iterations}]")

                # === Thought ===
                thought = self._generate_thought(query, context, iteration)
                context["thoughts"].append(thought)

                if verbose:
                    print(f"  ğŸ§  Thought: {thought}")

                # === Action ===
                action_decision = self._decide_action(thought, context)

                if action_decision["action"] == "answer":
                    # ç›´æ¥å›ç­”
                    answer = self._generate_final_answer(query, context)
                    context["final_answer"] = answer

                    if verbose:
                        print(f"  âœ… Decision: Generate final answer")

                    break

                elif action_decision["action"] == "search":
                    # æ‰§è¡Œæ£€ç´¢
                    search_query = action_decision.get("query", query)

                    if verbose:
                        print(f"  ğŸ” Action: Search for '{search_query}'")

                    observation = self._vector_search(search_query, mode=mode)
                    context["actions"].append("search")
                    context["observations"].append(observation)

                    if verbose:
                        print(f"  ğŸ“Š Observation: Found {len(observation.get('sources', []))} chunks")

                    # === Reflection: è¯„ä¼°æ£€ç´¢è´¨é‡ ===
                    reflection = self._reflect_on_search(query, observation, iteration)
                    context["reflections"].append(reflection)

                    if verbose:
                        print(f"  ğŸ” Reflection: {reflection['summary']}")

                    # å¦‚æœåæ€è®¤ä¸ºè´¨é‡ä¸å¥½ï¼Œè°ƒæ•´ç­–ç•¥ç»§ç»­æœç´¢
                    if not reflection["is_satisfactory"] and iteration < self.max_iterations:
                        if verbose:
                            print(f"  âš ï¸  Quality issue: {reflection['issue']}")
                            print(f"  ğŸ”„ Adjusting strategy for next search...")

            # å¦‚æœå¾ªç¯ç»“æŸè¿˜æ²¡æœ‰ç­”æ¡ˆï¼Œç”Ÿæˆä¸€ä¸ª
            if not context["final_answer"]:
                context["final_answer"] = self._generate_final_answer(query, context)

            return {
                "answer": context["final_answer"],
                "sources": self._extract_all_sources(context),
                "react_steps": len(context["thoughts"]),
                "reflections_count": len(context["reflections"]),
                "thoughts": context["thoughts"] if verbose else [],
                "tools_used": list(set(context["actions"])) if context["actions"] else []
            }

        # Run with timeout protection
        try:
            return run_with_timeout()
        except Exception as e:
            print(f"    [âš ï¸  ReAct loop error: {e}]")
            # Fallback: return whatever we have
            return {
                "answer": context.get("final_answer") or self._generate_final_answer(query, context),
                "sources": self._extract_all_sources(context),
                "react_steps": len(context["thoughts"]),
                "reflections_count": len(context["reflections"]),
                "thoughts": context["thoughts"] if verbose else [],
                "tools_used": list(set(context["actions"])) if context["actions"] else []
            }

    def _generate_thought(self, query: str, context: Dict, iteration: int) -> str:
        """ç”Ÿæˆæ€è€ƒï¼šåˆ†æå½“å‰æƒ…å†µï¼ˆæ”¯æŒLook-Backæœºåˆ¶å’Œä¸Šä¸‹æ–‡æ³¨å…¥ï¼‰"""

        previous_topic = context.get("previous_topic", "")

        # ç¬¬ä¸€æ¬¡è¿­ä»£ï¼šæ€»æ˜¯æœç´¢
        if iteration == 1:
            if previous_topic:
                # ã€ä¸Šä¸‹æ–‡æ³¨å…¥ã€‘ï¼šå‘Šè¯‰ Agent ä¸Šä¸€è½®çš„ä¸»é¢˜
                return f"I need to search for information about '{query}'. Note: {previous_topic}"
            else:
                return f"I need to search for information about {query}"

        # åç»­è¿­ä»£ï¼šåŸºäºåæ€è°ƒæ•´ç­–ç•¥
        if context["reflections"]:
            last_reflection = context["reflections"][-1]

            # å¦‚æœä¸Šæ¬¡æœç´¢è´¨é‡ä¸å¥½ï¼Œè°ƒæ•´ç­–ç•¥
            if not last_reflection.get("is_satisfactory", True):
                issue = last_reflection.get("issue", "")

                # Look-Back æœºåˆ¶ï¼šæ£€æŸ¥æŸ¥è¯¢æ˜¯å¦æœ‰ä»£è¯æˆ–æ¨¡ç³Šè¯
                if any(pronoun in query.lower() for pronoun in ["it", "they", "this", "that"]):
                    # ä¼˜å…ˆä½¿ç”¨ previous_topicï¼ˆæ¥è‡ªå¯¹è¯å†å²ï¼‰
                    if previous_topic:
                        # ä»ä¸»é¢˜ä¸­æå–å…³é”®è¯
                        topic_words = previous_topic.split(":")[-1].strip()
                        refined_query = f"{topic_words} {query}"
                        return f"The query '{query}' contains pronouns. {previous_topic}. I should search for '{refined_query}' instead."

                    # å¦‚æœæ²¡æœ‰ previous_topicï¼Œä»ä¹‹å‰çš„æœç´¢ç»“æœä¸­æå–å…³é”®è¯
                    if context["observations"]:
                        last_obs = context["observations"][-1]
                        last_answer = last_obs.get("answer", "")
                        keywords = self._extract_keywords(last_answer, top_n=3)
                        if keywords:
                            # ä¿®æ­£æŸ¥è¯¢è¯
                            refined_query = f"{keywords[0]} {query}"
                            return f"The query '{query}' was too vague. Based on context, I should search for '{refined_query}' instead."

                if issue == "semantic_mismatch":
                    # è¯­ä¹‰ä¸åŒ¹é…ï¼šä½¿ç”¨æŸ¥è¯¢æ‰©å±•ï¼Œå¹¶è€ƒè™‘ previous_topic
                    if previous_topic:
                        return f"The previous search found irrelevant content. {previous_topic}. Let me try searching for '{previous_topic.split(':')[-1].strip()} {query}'"
                    else:
                        expanded_query = self._expand_query(query, context)
                        return f"The previous search found irrelevant content. Let me try an expanded query: '{expanded_query}'"

                elif issue == "retrieval_failed":
                    # æ£€ç´¢å®Œå…¨å¤±è´¥ï¼šå°è¯•æŸ¥è¯¢æ‰©å±•
                    expanded_query = self._expand_query(query, context)
                    return f"The search found no relevant results (score < 0.1). Let me try with expanded terms: '{expanded_query}'"

                elif issue == "no_info_found":
                    # è¯´æ²¡æ‰¾åˆ°ä¿¡æ¯ï¼Œå°è¯•æ¢ä¸ªå…³é”®è¯
                    return f"The search didn't find relevant info. Let me try a more specific search for {query}"

                elif issue == "answer_too_short":
                    # ç­”æ¡ˆå¤ªçŸ­ï¼Œå¯èƒ½éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡
                    return f"The previous answer was too brief. Let me search with more context about {query}"

                elif issue == "content_truncated":
                    # å†…å®¹è¢«æˆªæ–­ï¼Œå°è¯•è·å–å®Œæ•´ä¿¡æ¯
                    return f"The content was incomplete. Let me search for the full definition of {query}"

                elif issue == "incomplete_comparison":
                    # ã€æ–°å¢ã€‘å¯¹æ¯”ä¸å®Œæ•´ï¼šå¼ºåˆ¶è¦æ±‚æ›´å…¨é¢çš„æœç´¢
                    return f"The search results seem incomplete for a comparison task (too few points found). I need to search specifically for a 'full comparison table' or more detailed differences regarding {query}."

                else:
                    # å…¶ä»–é—®é¢˜ï¼Œé‡æ–°æœç´¢
                    return f"Previous search had quality issues. Let me try searching for {query} with a different approach"

        # æ£€æŸ¥æœ€åä¸€æ¬¡æœç´¢çš„ç­”æ¡ˆè´¨é‡
        if context["observations"]:
            last_obs = context["observations"][-1]
            last_answer = last_obs.get("answer", "")

            # å¦‚æœæœ‰ç­”æ¡ˆä¸”é•¿åº¦è¶³å¤Ÿï¼Œå¯ä»¥è€ƒè™‘å›ç­”
            if len(last_answer) > 100:
                return "I have sufficient information from the search to answer"

        # é»˜è®¤ï¼šç»§ç»­æœç´¢
        return f"I need to search for more specific information about {query}"

    def _decide_action(self, thought: str, context: Dict) -> Dict:
        """å†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œ"""

        thought_lower = thought.lower()

        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰è¶³å¤Ÿä¿¡æ¯
        if "sufficient information" in thought_lower or "can answer" in thought_lower:
            return {"action": "answer"}

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æœç´¢
        if "need to search" in thought_lower or "search for" in thought_lower:
            # æå–æœç´¢å…³é”®è¯
            match = re.search(r'search for\s+(.+?)(?:\.|$)', thought_lower, re.IGNORECASE)
            if match:
                search_query = match.group(1).strip()
                return {"action": "search", "query": search_query}
            return {"action": "search", "query": context["query"]}

        # ç¬¬ä¸€æ¬¡è¿­ä»£é»˜è®¤æœç´¢
        if not context["observations"]:
            return {"action": "search", "query": context["query"]}

        # é»˜è®¤å›ç­”
        return {"action": "answer"}

    def _reflect_on_search(self, query: str, observation: Dict, iteration: int) -> Dict:
        """
        åæ€ï¼šè¯„ä¼°æ£€ç´¢è´¨é‡

        æ£€æŸ¥ï¼š
        1. æ£€ç´¢åˆ†æ•°æ˜¯å¦å¤ªä½ï¼ˆ< 0.1 è¡¨ç¤ºæ£€ç´¢å¤±è´¥ï¼‰
        2. ç­”æ¡ˆé•¿åº¦æ˜¯å¦è¶³å¤Ÿ
        3. æ˜¯å¦æœ‰ç¢ç‰‡æ ‡è®°ï¼ˆ"...", "ove..." ç­‰ï¼‰
        4. æ˜¯å¦åŒ…å«"æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯"çš„æç¤º

        Returns:
            {
                "is_satisfactory": bool,
                "summary": str,
                "issue": str (if not satisfactory)
            }
        """
        # æ£€æŸ¥ 0: æ£€ç´¢åˆ†æ•°å¤ªä½ï¼ˆæ£€ç´¢å¤±è´¥ï¼‰
        top_scores = observation.get("top_scores", [])
        if top_scores:
            try:
                # Parse scores (they might be strings like "0.8364")
                max_score = max(float(s) for s in top_scores)
                if max_score < 0.1:
                    return {
                        "is_satisfactory": False,
                        "summary": f"Retrieval failed: max score {max_score:.4f} < 0.1",
                        "issue": "retrieval_failed"
                    }
            except (ValueError, TypeError):
                pass  # If score parsing fails, continue with other checks

        answer = observation.get("answer", "")

        # æ£€æŸ¥ 1: ç­”æ¡ˆå¤ªçŸ­
        if len(answer) < 50:
            return {
                "is_satisfactory": False,
                "summary": "Answer too short",
                "issue": "answer_too_short"
            }

        # æ£€æŸ¥ 2: ç­”æ¡ˆåŒ…å«"æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯"
        negative_phrases = [
            "does not contain enough information",
            "does not have enough information",
            "no information available",
            "context does not contain"
        ]
        for phrase in negative_phrases:
            if phrase.lower() in answer.lower():
                return {
                    "is_satisfactory": False,
                    "summary": "Answer says no info found",
                    "issue": "no_info_found"
                }

        # æ£€æŸ¥ 3: ç­”æ¡ˆæœ‰ç¢ç‰‡æ ‡è®°
        fragmentation_markers = [
            "...",  # çœç•¥å·
            "ove...",  # overfitting è¢«æˆªæ–­
            "defin",  # definition è¢«æˆªæ–­
            "prevent",  # prevent è¢«æˆªæ–­
            "This is called",  # åé¢åº”è¯¥æœ‰å†…å®¹ä½†æ–­äº†
        ]
        for marker in fragmentation_markers:
            if marker in answer:
                # æ£€æŸ¥æ˜¯å¦åœ¨å¥å­æœ«å°¾ï¼ˆçœŸæ­£çš„æˆªæ–­ï¼‰
                if answer.endswith(marker) or answer.endswith(marker + "."):
                    return {
                        "is_satisfactory": False,
                        "summary": "Content appears truncated",
                        "issue": "content_truncated"
                    }

        # æ£€æŸ¥ 4: ç­”æ¡ˆè´¨é‡åŸºäºæ¥æºæ•°é‡
        sources = observation.get("sources", [])
        if len(sources) == 0:
            return {
                "is_satisfactory": False,
                "summary": "No sources retrieved",
                "issue": "no_sources"
            }

        # æ£€æŸ¥ 5: è¯­ä¹‰ç›¸å…³æ€§æ ¡éªŒï¼ˆæ–°å¢ï¼‰
        # é¿å…ç”¨ç©ºæ°”åŠ¨åŠ›å­¦å›ç­”æ•°æ®åˆ‡åˆ†é—®é¢˜
        semantic_check = self._check_semantic_relevance(query, observation)
        if not semantic_check["is_satisfactory"]:
            return semantic_check

        # =========================================================
        # æ£€æŸ¥ 6: é’ˆå¯¹å¯¹æ¯”/åˆ—è¡¨ç±»é—®é¢˜çš„å®Œæ•´æ€§æ£€æŸ¥ (Completeness Check)
        # =========================================================
        is_comparison = any(w in query.lower() for w in ["compare", "difference", "vs", "versus", "distinction", "list", "types of"])

        if is_comparison:
            # ç»Ÿè®¡ç»“æ„åŒ–æ ‡è®°ï¼šMarkdownè¡¨æ ¼(|), åˆ—è¡¨é¡¹(-, *)
            # å¦‚æœå†…å®¹è™½ç„¶é•¿ï¼Œä½†åªæ˜¯ä¸€å¤§æ®µåºŸè¯ï¼Œæ²¡æœ‰åˆ†ç‚¹ï¼Œå¯¹äºå¯¹æ¯”é¢˜æ¥è¯´ä¹Ÿæ˜¯ä¸åˆæ ¼çš„
            structure_score = answer.count("|") + answer.count("\n-") + answer.count("\n*") + answer.count("\n1.")

            # é˜ˆå€¼è®¾å®šï¼š
            # 1. å¦‚æœåŒ…å«è¡¨æ ¼ç¬¦å· '|' å°‘äº 4 ä¸ªï¼ˆè¯´æ˜è¿è¡¨å¤´éƒ½æ²¡æœ‰ï¼‰ï¼Œä¸”åˆ—è¡¨é¡¹å°‘äº 3 ä¸ª
            # 2. å¹¶ä¸”è¿˜æ²¡è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆç»™å®ƒé‡è¯•çš„æœºä¼šï¼‰
            if structure_score < 3 and iteration < self.max_iterations:
                return {
                    "is_satisfactory": False,
                    "summary": f"Potential incomplete comparison (score: {structure_score})",
                    "issue": "incomplete_comparison"  # æ–°çš„ issue ç±»å‹
                }

        # é€šè¿‡æ‰€æœ‰æ£€æŸ¥
        return {
            "is_satisfactory": True,
            "summary": f"Good quality: {len(answer)} chars, {len(sources)} sources"
        }

    def _generate_final_answer(self, query: str, context: Dict) -> str:
        """
        ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼ˆæ™ºèƒ½å›é€€ç­–ç•¥ï¼‰

        ä¿è¯ï¼šAgentic RAG çš„æ•ˆæœè‡³å°‘ä¸ä½äº Enhanced RAG
        æ”¯æŒå¸¦æ ‡ç­¾çš„ä¸Šä¸‹æ–‡æ‹¼æ¥ï¼ˆç”¨äºæŸ¥è¯¢æ‹†è§£åœºæ™¯ï¼‰
        """
        # æ”¶é›†æ‰€æœ‰æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆå¸¦æ ‡ç­¾çš„æ‹¼æ¥ï¼‰
        all_context = []
        chunk_idx = 1
        was_decomposed = context.get("sub_queries") is not None and len(context.get("sub_queries", [])) > 1

        for obs in context["observations"]:
            if "sources" in obs:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ‹†è§£æŸ¥è¯¢çš„å­æŸ¥è¯¢ç»“æœ
                sub_query_label = obs.get("sub_query", "")
                if was_decomposed and sub_query_label:
                    # å¸¦æ ‡ç­¾çš„ä¸Šä¸‹æ–‡æ‹¼æ¥ï¼ˆå¸®åŠ©LLMç†è§£æ¥æºï¼‰
                    all_context.append(f"\n## Results for: {sub_query_label}\n")

                # Use up to 5 chunks from each observation for more context
                for source in obs["sources"][:5]:
                    content = source.get("content", source)
                    # Clean up the content (remove excessive whitespace)
                    content = " ".join(content.split())[:500]  # Truncate very long chunks

                    if was_decomposed and sub_query_label:
                        # å¸¦æ ‡ç­¾çš„chunk
                        all_context.append(f"[{chunk_idx}] {content}")
                    else:
                        # æ™®é€šchunk
                        all_context.append(f"[{chunk_idx}] {content}")
                    chunk_idx += 1

        context_str = "\n".join(all_context) if all_context else "No context available"

        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ£€æµ‹å¯¹æ¯”ç±»é—®é¢˜ï¼Œæ³¨å…¥å¼ºåˆ¶å®Œæ•´æ€§çš„æŒ‡ä»¤
        is_comparison_query = any(w in query.lower() for w in ["compare", "difference", "vs", "versus", "distinction"])

        # æ”¹è¿›çš„Prompt - æ”¯æŒå¯¹æ¯”ç±»é—®é¢˜
        if was_decomposed:
            # æ‹†è§£æŸ¥è¯¢çš„ä¸“ç”¨prompt
            prompt = f"""You are a helpful assistant. The user's question was broken down into multiple sub-queries, and the results from each are provided below.

User Question: {query}

The question was decomposed into:
{chr(10).join(f'- {sq}' for sq in context.get('sub_queries', []))}

Context from each sub-query:
{context_str}

Instructions:
1. Synthesize information from ALL sub-queries to answer the original question
2. For comparison questions, clearly state the differences between each aspect
3. Use all relevant information from the context
4. Be thorough and complete

Answer:"""
        else:
            # æ ‡å‡†prompt - é’ˆå¯¹å¯¹æ¯”ç±»é—®é¢˜åŠ å¼ºæŒ‡ä»¤
            if is_comparison_query:
                # å¯¹æ¯”ç±»é—®é¢˜çš„å¼ºåŒ– prompt
                prompt = f"""Based on the following retrieved context, answer the user's question.

User Question: {query}

Context:
{context_str}

Instructions:
1. Answer the question using ONLY the provided context.
2. If the context doesn't contain enough information, say so.
3. Be concise and direct.
4. Include specific details from the context when relevant.

CRITICAL RULES FOR COMPARISON/LISTS:
- If the user asks to COMPARE items (e.g., "difference", "vs"), you MUST list ALL differences found in the context.
- Do NOT summarize or pick just one point. Be comprehensive.
- If the context contains a TABLE (marked by '|'), please reconstruct the table in your answer or list every row clearly.

Answer:"""
            else:
                # æ™®é€šé—®é¢˜çš„æ ‡å‡† prompt
                prompt = f"""You are a helpful assistant. Based on the following retrieved context, answer the user's question thoroughly.

User Question: {query}

Context:
{context_str}

Instructions:
1. Use the provided context to answer the question
2. If the context contains relevant code examples, include them
3. If the context doesn't contain enough information to fully answer, still provide what you can from the context
4. Be clear and concise, but complete
5. Don't say "the context doesn't mention" - just use what's available

Answer:"""

        # Try Agent generation with increased token limit
        agent_answer = ""
        try:
            headers = {"Authorization": f"Bearer {self.glm_key}"}
            payload = {
                "model": RAGConfig.CHAT_MODEL,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 2000,  # Increased from 1000 to allow longer answers
                "temperature": 0.7
            }
            res = requests.post(self.api_url, headers=headers, json=payload, timeout=60)  # 60s timeout for generation
            res.raise_for_status()
            data = res.json()

            if "choices" in data:
                agent_answer = data["choices"][0]["message"]["content"].strip()

        except Exception as e:
            print(f"    [Agent generation error: {e}]")

        # ============================================================
        # æ™ºèƒ½å›é€€é€»è¾‘ï¼šä¿è¯ä¸ä½äº Enhanced RAG çš„è´¨é‡
        # ============================================================

        # Define what's unsatisfactory
        is_agent_unsatisfactory = (
            not agent_answer or                                    # Empty answer
            len(agent_answer) < 50 or                               # Too short (lowered from 100)
            "does not contain enough information" in agent_answer.lower() or  # Refusal
            "context does not contain" in agent_answer.lower()      # Refusal (new check)
        )

        if is_agent_unsatisfactory:
            print(f"    [âš ï¸  Agent answer unsatisfactory ({len(agent_answer)} chars). Using Pipeline fallback...")

            # Check if we have Enhanced RAG (Pipeline) answer available
            if context["observations"]:
                # Get the best pipeline answer (last observation is most recent)
                pipeline_answer = context["observations"][-1].get("answer", "")

                # Fallback if pipeline answer is reasonable (lowered threshold from 150 to 50)
                if len(pipeline_answer) > 50:
                    print(f"    [âœ… Fallback to Pipeline: {len(pipeline_answer)} chars]")
                    return pipeline_answer

        # If Agent answer is satisfactory, use it
        if agent_answer:
            return agent_answer

        # Final fallback
        return "Sorry, I couldn't find relevant information to answer this question."

    def _direct_query(self, query: str, mode: str) -> Dict:
        """ç›´æ¥æŸ¥è¯¢ï¼ˆä¸ä½¿ç”¨ ReActï¼‰"""
        result = self.pipeline.run(query, mode=mode)

        return {
            "answer": result.get("answer", ""),
            "sources": result.get("retrieved_chunks", []),
            "react_steps": 0,
            "thoughts": [],
            "tools_used": ["vector_search"]
        }

    def _vector_search(self, query: str, mode: str = "hybrid") -> Dict:
        """å‘é‡æ£€ç´¢å·¥å…·"""
        result = self.pipeline.run(query, mode=mode)

        return {
            "answer": result.get("answer", ""),
            "sources": result.get("retrieved_chunks", []),
            "method": result.get("method", "")
        }

    def _bm25_search(self, query: str, **kwargs) -> Dict:
        """BM25 æ£€ç´¢å·¥å…·"""
        result = self.pipeline.run(query, mode="bm25_only")

        return {
            "answer": result.get("answer", ""),
            "sources": result.get("retrieved_chunks", []),
            "method": result.get("method", "")
        }

    def _extract_all_sources(self, context: Dict) -> List:
        """æå–æ‰€æœ‰æ¥æº"""
        all_sources = []
        seen = set()

        for obs in context["observations"]:
            if "sources" in obs:
                for source in obs["sources"]:
                    # å»é‡
                    source_id = source.get("chunk_id", id(source))
                    if source_id not in seen:
                        seen.add(source_id)
                        all_sources.append(source)

        return all_sources

    def _get_conversation_history(self, k: int = 3) -> List[Dict]:
        """è·å–å¯¹è¯å†å²"""
        if not self.memory:
            return []

        memories = self.memory.short_term.get_recent(k)
        return [
            {
                "query": m.query,
                "answer": m.answer
            }
            for m in memories
        ]

    def _save_to_memory(self, original_query: str, rewritten_query: str,
                       answer: str, tools_used: List[str], importance: float):
        """ä¿å­˜åˆ°è®°å¿†"""
        # è·å– embedding
        try:
            embedding = self.pipeline.qianfan_client.embed([rewritten_query + " " + answer])
            if len(embedding) > 0:
                answer_embedding = embedding[0]
            else:
                answer_embedding = None
        except:
            answer_embedding = None

        # ä¿å­˜åˆ°çŸ­æœŸè®°å¿†
        self.memory.add_memory(
            query=original_query,  # ä¿å­˜åŸå§‹æŸ¥è¯¢
            answer=answer,
            tools_used=tools_used,
            embedding=answer_embedding,
            importance=importance
        )

        # é«˜é‡è¦æ€§å­˜å…¥é•¿æœŸè®°å¿†
        if importance >= 0.7 and answer_embedding is not None:
            self.memory.add_memory(
                query=original_query,
                answer=answer,
                tools_used=tools_used,
                embedding=answer_embedding,
                importance=importance,
                to_long_term=True
            )

    # ========== ä¾¿æ·æ–¹æ³• ==========

    def chat(self, message: str, **kwargs) -> str:
        """ç®€åŒ–çš„èŠå¤©æ¥å£"""
        result = self.query(message, verbose=False, **kwargs)
        return result["answer"]

    def get_memory_stats(self) -> Dict:
        """è·å–è®°å¿†ç»Ÿè®¡"""
        if not self.memory:
            return {"memory_enabled": False}

        stats = self.memory.get_stats()
        stats["memory_enabled"] = True
        return stats

    def clear_memory(self):
        """æ¸…ç©ºçŸ­æœŸè®°å¿†"""
        if self.memory:
            self.memory.short_term.clear()

    def get_conversation_history(self, k: int = 5) -> List[Dict]:
        """è·å–å¯¹è¯å†å²"""
        if not self.memory:
            return []

        memories = self.memory.short_term.get_recent(k)
        return [
            {
                "query": m.query,
                "answer": m.answer,
                "timestamp": m.timestamp,
                "tools": m.tools_used
            }
            for m in memories
        ]


# ============================================
# ä¾¿æ·æ¥å£
# ============================================
def create_react_agent(pipeline: RAGPipeline, glm_key: str,
                      enable_memory: bool = True,
                      enable_react: bool = True) -> ReActAgent:
    """åˆ›å»º ReAct Agent çš„ä¾¿æ·å‡½æ•°"""
    return ReActAgent(
        pipeline=pipeline,
        glm_key=glm_key,
        enable_memory=enable_memory,
        enable_react=enable_react
    )
